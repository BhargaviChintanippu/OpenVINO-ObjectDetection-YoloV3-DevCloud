{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Object detection Yolo V3 Python Demo with Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "This demo showcases Object Detection with YOLO* V3 and Async API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import Dependicies\n",
    "\n",
    "Execute the following cell to import Python dependencies needed for displaying the results in this notebook (tip: select the cell and use Ctrl+Enter to execute the cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys                                     \n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "sys.path.insert(0,os.path.join(os.environ['HOME'],'Reference-samples/iot-devcloud/demoTools/'))\n",
    "from demoutils import *\n",
    "from openvino.inference_engine import IEPlugin, IENetwork\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Source your environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Build the OpenVINO Samples\n",
    "\n",
    "Execute the following cell to build the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/inference_engine/samples/build_samples.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "### 1.  Converting YOLO* Models to the Intermediate Representation (IR) using Model Optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the following commands to Dump YOLOv3 TensorFlow* Model\n",
    "\n",
    "##### 1.1  Clone the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mystic123/tensorflow-yolo-v3.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 (Optional) Checkout to the commit that the conversion was tested on:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd tensorflow-yolo-v3 && git checkout ed60b90\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Download the yolov3.weights (for the YOLOv3 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://pjreddie.com/media/files/yolov3.weights -P $HOME/Benchmarking_DL_Models/Yolo_V3_Model/tensorflow-yolo-v3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Download coco.names file from the DarkNet website OR use labels that fit your task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names -P $HOME/Benchmarking_DL_Models/Yolo_V3_Model/tensorflow-yolo-v3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Run the following cell to generate .pb File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 tensorflow-yolo-v3/convert_weights_pb.py --class_names tensorflow-yolo-v3/coco.names --data_format NHWC --weights_file tensorflow-yolo-v3/yolov3.weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Run the following command to generate .xml and .bin files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir FP32,FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model=$HOME/Benchmarking_DL_Models/Yolo_V3_Model/frozen_darknet_yolov3_model.pb --tensorflow_use_custom_operations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/yolo_v3.json --batch 1 -o FP32/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model=$HOME/Benchmarking_DL_Models/Yolo_V3_Model/frozen_darknet_yolov3_model.pb --tensorflow_use_custom_operations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/yolo_v3.json --batch 1 --data_type=FP16 -o FP16/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Video\n",
    "\n",
    "Run the following cell to get the input video from the Reference samples directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $HOME/Reference-samples/iot-devcloud/cpp/interactive_face_detection_demo/faces-recognition-walking.mp4 $HOME/Benchmarking_DL_Models/Yolo_V3_Model/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to create a symlink and view the input video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf $HOME/Benchmarking_DL_Models/Yolo_V3_Model/ \n",
    "videoHTML('Input Video', ['faces-recognition-walking.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to view the parameters which are used to run the application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 object_detection_demo_yolov3_async.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inference on a video\n",
    "\n",
    "The Python code takes in command line arguments for video, model and so on.\n",
    "\n",
    "**Command line arguments options and how they are interpreted in the application source code**\n",
    "\n",
    "```\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 object_detection_demo_yolov3_async.py  -m ${MODELPATH} \\\n",
    "                                               -i ${INPUT_FILE} \\\n",
    "                                               -o ${RESULTS_PATH} \\\n",
    "                                               -d ${DEVICE} \\\n",
    "                                               --labels ${LABEL_FILE} \\\n",
    "                                               -l /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\n",
    "\n",
    "```\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* -m location of the **Yolo V3** Model\n",
    "* -i location of the input video stream\n",
    "* -o location where the output file with inference needs to be stored (results/[device])\n",
    "* -d type of Hardware Acceleration (CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "* --labels Labels mapping File\n",
    "* -l absolute path to the shared library and is currently optimized for core/xeon (/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Create a Job File\n",
    "\n",
    "All the code up to this point has been executed within the Jupyter Notebook instance running on a development node based on an Intel® Xeon® Scalable Processor, where the Notebook is allocated to a single core. To run inference on the entire video, you need more compute power. Run the workload on several DevCloud's edge compute nodes and then send work to the edge compute nodes by submitting jobs into a queue. For each job, specify the type of the edge compute server that must be allocated for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass the specific variables to the Python code, we use the following arguments:\n",
    "\n",
    "* `-m`       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;location of the optimized **YOLO V3** model's XML\n",
    "* `-i`       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;location of the input video\n",
    "* `-o`       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output directory\n",
    "* `-d`       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hardware device type (CPU, GPU, MYRIAD)\n",
    "* `-l`       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path to the CPU extension library\n",
    "* `--labels` &nbsp;&nbsp;&nbsp;&nbsp;Labels mapping File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job file will be executed directly on the edge compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile obj_det_yolo_job.sh\n",
    "ME=`basename $0`\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "INPUT_FILE=$4\n",
    "RESULTS_BASE=$1\n",
    "\n",
    "\n",
    "MODELPATH=\"$HOME/Benchmarking_DL_Models/Yolo_V3_Model/${FP_MODEL}/frozen_darknet_yolov3_model.xml\"\n",
    "RESULTS_PATH=\"${RESULTS_BASE}\"\n",
    "mkdir -p $RESULTS_PATH\n",
    "echo \"$ME is using results path $RESULTS_PATH\"\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/altera/aocl-pro-rte/aclrte-linux64/\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    source /opt/fpga_support_files/setup_env.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_bitstreams/2019R1_PL1_FP11_MobileNet_Clamp.aocx\n",
    "fi\n",
    "\n",
    "# Running the object detection code\n",
    "! python3 object_detection_demo_yolov3_async.py  -m ${MODELPATH} \\\n",
    "                                                 -i $INPUT_FILE \\\n",
    "                                                 -o $RESULTS_PATH \\\n",
    "                                                 -d $DEVICE \\\n",
    "                                                 --labels tensorflow-yolo-v3/coco.names \\\n",
    "                                                 -l /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit people_counter to several different types of edge compute nodes simultaneously or just one node at a time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option let us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option let us send arguments to the bash script. \n",
    "- `-N` : this option let us name the job so that it is easier to distinguish between them.\n",
    "\n",
    "The `-F` flag is used to pass arguments to the job script.\n",
    "The [obj_det_yolo_job.sh](obj_det_yolo_job.sh) takes in 4 arguments:\n",
    "1. the path to the directory for the output video and performance stats\n",
    "2. targeted device (e.g. CPU, GPU and MYRIAD.\n",
    "3. the floating precision to use for inference\n",
    "4. location of the input video stream\n",
    "\n",
    "The job scheduler uses the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "**Note**: If you want to use your own video, change the environment variable 'VIDEO' in the following cell from \"input_video.mp4\" to the full path of your uploaded video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VIDEO\"] = 'faces-recognition-walking.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Job queue submission\n",
    "\n",
    "Each of the cells below will submit a job to different edge compute nodes.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)\n",
    "\n",
    "#### Submitting to an edge compute node with an Intel® CPU\n",
    "In the cell below, submit a job to <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core™ i5-6500TE processor</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub obj_det_yolo_job.sh -l nodes=1:idc001skl:i5-6500te -F \"results/Core CPU FP32 $VIDEO \" -N obj_det_core\n",
    "print(job_id_core[0]) \n",
    "#Progress indicator\n",
    "if job_id_core:\n",
    "    progressIndicator('results/Core', 'i_progress.txt', \"Inference\", 0, 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above progress Indicator, Inferencing is taking more time because along with inferencing,it is parsing and rendering simultaneously for each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core CPU and using the onboard Intel® GPU\n",
    "In the following cell, we submit a job to <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub obj_det_yolo_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \" results/GPU GPU FP32 $VIDEO\" -N obj_det_gpu \n",
    "print(job_id_gpu[0])\n",
    "#Progress indicator\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/GPU', 'i_progress.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above progress Indicator, Inferencing is taking more time because along with inferencing,it is parsing and rendering simultaneously for each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® NCS 2 (Neural Compute Stick 2)\n",
    "In the following cell, we submit a job to <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_ncs2 = !qsub obj_det_yolo_job.sh -l nodes=1:idc004nc2:intel-ncs2 -F \"results/NCS2 MYRIAD FP16 $VIDEO \" -N obj_det_ncs2\n",
    "print(job_id_ncs2[0]) \n",
    "#Progress indicator\n",
    "if job_id_ncs2:\n",
    "    progressIndicator('results/NCS2', 'i_progress.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above progress Indicator, Inferencing is taking more time because along with inferencing,it is parsing and rendering simultaneously for each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with IEI Mustang-V100-MX8 ( Intel® Movidius™ Myriad™ X Vision Processing Unit (VPU))\n",
    "In the cell below, we submit a job to an IEI Tank 870-Q170 edge node with an Intel Core i5-6500te CPU. The inference workload will run on an IEI Mustang-V100-MX8 accelerator installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_hddlr = !qsub obj_det_yolo_job.sh -l nodes=1:idc002mx8:iei-mustang-v100-mx8 -F \"results/HDDL HDDL FP16 $VIDEO\" -N obj_det_hddlr\n",
    "print(job_id_hddlr[0]) \n",
    "#Progress indicator\n",
    "if job_id_hddlr:\n",
    "    progressIndicator('results/HDDL', 'i_progress.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above progress Indicator, Inferencing is taking more time because along with inferencing,it is parsing and rendering simultaneously for each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check the Progress\n",
    "\n",
    "Check the progress of the jobs. `Q` status stands for `queued`, `R` for `running`. How long a job is being queued is dependent on number of the users. It should take up to 5 minutes for a job to run. If the job is no longer listed, it's done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 3.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 3: View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the `stdout` and `stderr` streams of each job into files with names\n",
    "`obj_det_{type}.o{JobID}` and `obj_det_{type}.e{JobID}`. Here, obj_det_{type} corresponds to the `-N` option of qsub. For example, `core` for Core CPU target.\n",
    "\n",
    "You can find the output video files inside the `results` directory. We wrote a short utility script that will display these videos within the notebook. See `demoutils.py` if you are interested in understanding further how the results are displayed in notebook. \n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "(here, obj_det_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "However, for this case, we may be more interested in the output video files. They are stored in mp4 format inside the `results/` directory.\n",
    "We wrote a short utility script that will display these videos with in the notebook.\n",
    "Run the cells below to display them.\n",
    "See `demoutils.py` if you are interested in understanding further how the results are displayed in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank (Intel Core CPU)', \n",
    "          ['results/Core/output.mp4'], \n",
    "          'results/Core/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Intel GPU (Intel Core + Onboard GPU)', \n",
    "          ['results/GPU/output.mp4'],\n",
    "          'results/GPU/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Intel CPU + Intel NCS2',\n",
    "           ['results/NCS2/output.mp4'], \n",
    "          'results/NCS2/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + IEI Mustang-V100-MX8 ( Intel® Movidius™ Myriad™ X Vision Processing Unit (VPU))',\n",
    "          ['results/HDDL/output.mp4'],\n",
    "         'results/HDDL/stats.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Performance Comparison\n",
    "\n",
    "The running time of each inference task is recorded in `stats.txt` in `results` folder, where the *architecture* corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values for processing time mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('gpu', 'GPU', ' Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('ncs2', 'NCS2', 'Intel\\nNCS2'),\n",
    "             ('hddlr','HDDL', ' IEI Mustang\\nV100-MX8\\nVPU')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, dir_,  a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/{}/stats.txt'.format(dir_), a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, seconds', 'Inference Engine Processing Time', 'time' )\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
